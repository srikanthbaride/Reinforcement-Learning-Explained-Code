{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ed6e6c",
   "metadata": {},
   "source": [
    "\n",
    "# TD Control Demo â€” SARSA vs Q-Learning (Cliff-Walking)\n",
    "\n",
    "This notebook runs **SARSA** (on-policy) and **Q-learning** (off-policy) on a self-contained **Cliff-Walking** environment and plots learning curves.  \n",
    "It uses the `ch7_td_control` package you just downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab or another environment, ensure the folder is on sys.path\n",
    "import sys, os\n",
    "pkg_dir = '/mnt/data'  # adjust if needed, e.g., '.' if `ch7_td_control/` is in the working dir\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.insert(0, pkg_dir)\n",
    "\n",
    "# Verify package location\n",
    "!ls -la /mnt/data | head -n 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ch7_td_control.cliff_env import CliffWalkingEnv\n",
    "from ch7_td_control.policies import fixed_epsilon, linear_decay, exp_decay, inverse_glie\n",
    "from ch7_td_control.sarsa import sarsa\n",
    "from ch7_td_control.q_learning import q_learning\n",
    "from ch7_td_control.plot_utils import plot_learning_curves\n",
    "\n",
    "episodes = 500\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "seed = 0\n",
    "\n",
    "# Choose an exploration schedule here:\n",
    "# sched = fixed_epsilon(0.1)\n",
    "# sched = linear_decay(eps0=0.3, eps_min=0.05, T=episodes)\n",
    "# sched = exp_decay(eps0=0.3, k=0.01, eps_min=0.05)\n",
    "sched = inverse_glie(c=1.0)  # GLIE-like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ff403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = CliffWalkingEnv(seed=seed)\n",
    "\n",
    "sarsa_Q, sarsa_returns = sarsa(env, episodes=episodes, alpha=alpha, gamma=gamma, eps_schedule=sched, seed=seed)\n",
    "ql_Q, ql_returns       = q_learning(env, episodes=episodes, alpha=alpha, gamma=gamma, eps_schedule=sched, seed=seed)\n",
    "\n",
    "plot_learning_curves(list(range(episodes)), sarsa_returns, ql_returns, ma_k=20,\n",
    "                     title=\"Cliff-Walking: SARSA (safer transients) vs Q-learning (greedy target)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22de3b",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize Greedy Paths from the Learned Q-tables\n",
    "\n",
    "Below we extract greedy policies from the learned Q-tables and roll them out.  \n",
    "`A` = agent, `S` = start, `G` = goal, `X` = cliff cells, `*` = visited path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def greedy_action(Q: Dict[Tuple[Tuple[int,int], int], float], s, n_actions=4):\n",
    "    best_a, best_q = 0, float(\"-inf\")\n",
    "    for a in range(n_actions):\n",
    "        q = Q.get((s,a), 0.0)\n",
    "        if q > best_q:\n",
    "            best_q, best_a = q, a\n",
    "    return best_a\n",
    "\n",
    "def rollout_greedy(env, Q, max_steps=200):\n",
    "    s = env.reset()\n",
    "    path = [s]\n",
    "    for _ in range(max_steps):\n",
    "        a = greedy_action(Q, s, n_actions=env.action_space.n)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        path.append(s)\n",
    "        if done:\n",
    "            break\n",
    "    return path\n",
    "\n",
    "# Rollout & render\n",
    "env = CliffWalkingEnv(seed=seed)\n",
    "path_sarsa = rollout_greedy(env, sarsa_Q)\n",
    "print(\"SARSA greedy rollout:\")\n",
    "env.render(path_sarsa)\n",
    "\n",
    "env = CliffWalkingEnv(seed=seed)\n",
    "path_ql = rollout_greedy(env, ql_Q)\n",
    "print(\"Q-learning greedy rollout:\")\n",
    "env.render(path_ql)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
